# Advanced usage of LUMI-O  

!!! note
	The examples here assume that you have proprely configured the tools to use LUMI-O,
	otherwise they will default to using amazon aws s3.


## Moving tool configuration files

In some cases it might be required to read
credentials from some other location then the default 
locations under home. This can be achived using environment variables or command line flags.


|      | rclone        | s3cmd                  | aws                                         |
|------|---------------|------------------------|---------------------------------------------|
| ENV  | `RCLONE_CONFIG` | `S3CMD_CONFIG`           | `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` |
| FLAG | `--config FILE` | `-c FILE`, `--config=FILE` |                                             |


## Presigned URLs

Presigned URLs are URLs generated by the user which grant time-limited "public" access 
to and object. It's also possible to generate an URL which allows time-limited 
uppload for a specific object (key) in a bucket. 

### Read-only presigned urls

You can generate a presigned url using e.g s3cmd 

```bash
s3cmd signurl s3://example-bucket/file.txt <unix epoch time>
```

To get the required unix epoch time, it's possible to use online calculators,
e.g when one wants to grant access until a specific data, or then adding 
the desired duration to the current time.

```
s3cmd signurl s3://example-bucket/file.txt $(echo "`date +%s` + 3600 * 24 * <days valid> ")
```

Irregardless of the set expiry time, presigned urls will become invalid when
the access key used for the signing expires.  

It's also possible to use the `aws` command to presign:

```bash
aws presign s3://example-bucket/image.jpg --expires in <seconds>
```

### Writable presigned urls


There is no way to create presigned urls for `PUT` operations 
using either `s3cmd` or `aws`, below is a short example script
using boto3 to generate a valid url.

!!! note 
	You will need a sufficently new version (e.g 1.26 which is installed if using python3.6 is to old)  of boto3 for it
	to understand a default profile set in ~/.aws/credentials and corresponding config file 
	otherwise the tool will always default to aws s3 endpoint and you will need to specify the
	profile/endpoint when constructing the client.

``` bash
python3 presign.py presign file.txt
curl -X PUT -T presign.py "<generated url>"
```


**presign.py**
```python
import boto3
import argparse

def generate_presigned_url(s3_client, client_method, method_parameters, expires_in):
    try:
        url = s3_client.generate_presigned_url(
            ClientMethod=client_method, Params=method_parameters, ExpiresIn=expires_in
        )
    except:
        print("Couldn't get a presigned URL")
        raise
    return url

def usage_demo():

    parser = argparse.ArgumentParser()
    parser.add_argument("bucket", help="The name of the bucket.")
    parser.add_argument("key", help="The name of the bucket")
    args = parser.parse_args()
    s3_client = boto3.client("s3")
    client_action = "put_object"
    url = generate_presigned_url(
        s3_client, client_action, {"Bucket": args.bucket, "Key": args.key}, 1000
    )
    print(f"Generated put_object url: {url}")


if __name__ == "__main__":
    usage_demo()
```

## Granular Access management

Using the rclone config generated by `lumio-conf` or using `s3cmd put -P` you can easily
make objects and buckets public or private. This section
explains how to apply more granular rules than fully private/public to e.g:

- Share data with a another lumi project.
- Restrict object access to specific IP:s
- Make it a bit harder to accidentally delete data
- Allow external modification to just specific objects. 

Projects in LUMI-O are single user tenants/accounts where the tenant/account
names and project names are both the numerical id for the project, 
i.e 465000001 . 

Subsequently, all members of a LUMI-O project have the exact same 
rights and permissions, unlike on the LUMI filesystem where files have individual owners.
Keep this mind if you have critical data in LUMI-O as any other member of your
LUMI project could accidentally delete it 

!!! warning
	Be very carefully when configuring and updating access to buckets and objects.   
	It's possible to lock yourself out from your own data. Or alternatively make
	objects visible to the whole world. 


### ACLs vs Policies 

There are two ways to manage access for data in LUMI-O:

1. Policies
2. Access control list

While ACLs are simpler to configure, they are are an older
method for access control and offer much less granular control
over permissions. **We recommend primarily using Policies**  

Some other differences include:
- ACLs can only be used to allow more access, not restrict access from the default
- ACLs can be applied to to buckets and objects while policies can only be applied to buckets
	- Though you can create bucket policies which only affect specific objects in the bucket. 
	- This also means that you will have to individually / recursively apply ACL changes to all objects in a bucket + the bucket itself.


### Configuring Policies

You can apply policies to a bucket using `s3cmd` or `aws` commands:

```
s3cmd setpolicy policy.json  s3://<bucket_name>/
```

_policy.json_
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": ["s3:GetObject"],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::myBucket/object1",
      "Principal": {
                "AWS": [ 
                      "arn:aws:iam::<proj_id>:user/<proj_id>"
        ]
      }
    }
  ]
}
```


This would give `project_<proj_id>` the permission to download
`myBucket/object1`, but no e.g list other objects in the bucket.


#### Stop deletion

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "statement1",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/Dave"
      },
      "Action": [
        "s3:GetObjectVersion",
        "s3:GetBucketAcl"
      ],
      "Resource": [
        "arn:aws:s3:::DOC-EXAMPLE-BUCKET1",
	 	"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*"
      ]
    },
    {
      "Sid": "statement2",
      "Effect": "Deny",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/Dave"
      },
      "Action": [
        "s3:DeleteObject",
        "s3:DeleteObjectVersion",
        "s3:PutLifecycleConfiguration"
      ],
      "Resource": [
        "arn:aws:s3:::DOC-EXAMPLE-BUCKET1",
	    "arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*"
      ]
    }
  ]
}
```

### Configuring Access control lists (ACLs): 

```bash
s3cmd setacl --recursive --acl-grant=read:'<proj_id>$<proj_id>' s3://<bucket_name>/
```
Would grant read access to all objects in the `<bucket_name>` bucket. 
The single quotes are important as otherwise the shell might interpret `$<proj_id>` as an (empty) variable

ACLs  

!!! info
	The lumi-pub rlcone remotes configured by lumio-conf uses acl settings to make
	created objects and buckets public, and the same goes for `s3cmd put -P`
	So if you need to "unpublish" or "publish" some data you can use the above commands

### Accessing data under other projects.


The authentication information used when interacting with LUMI-O
partially defines the scope for buckets.

Public buckets/objects for a project are located under

`https://<proj_id>.lumidata.eu/<bucket>/<object>`


But making the request to the same url while authenticated will
try to fetch `<bucket>` from your own project not `proj_id`. 

Instead the format `https://lumidata.eu/<proj_id>:<bucket>/<object>`
must be used.


**s3cmd and rclone**

To access buckets and subsequently objects not owned
For both `s3cmd` and `rclone` 


```
s3cmd ls s3://<proj_id>:bucket/
```

```
rclone ls lumi-462000007-guest:"<proj_id>:bucket"
```

**Curl**

Don't use curl unless you have to, main
point here is that the project id has to be included
with the bucket and object name when generating the signature.

```
object=README.md
bucket=myBucket
project=465000001
resource="/$project:$bucket/$object"
endPoint=https://lumidata.eu$resource


contentType="text/plain"
dateValue=`date -R`
stringToSign="GET\n\n${contentType}\n${dateValue}\n${resource}"
s3Key=$S3_ACCESS_KEY_ID
s3Secret=$S3_SECRET_ACCESS_KEY
signature=`echo -en ${stringToSign} | openssl sha1 -hmac ${s3Secret} -binary | base64`
echo "Fetching object from $endPoint with authentication for 465000454"
curl -X GET -s -o out.tmp -w "%{http_code}"  \
     -H "Host: https://lumidata.eu/" \
     -H "Date: ${dateValue}" \
     -H "Content-Type: ${contentType}" \
     -H "Authorization: AWS ${s3Key}:${signature}" \
     $endPoint
```
